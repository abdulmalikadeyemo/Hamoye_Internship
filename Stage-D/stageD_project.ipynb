{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589465d3-0c7a-4c46-ac1f-f88a5554ecb8",
   "metadata": {},
   "source": [
    "# **Understanding the Amazon from Space Kaggle Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c924d1-a139-4e19-963f-4a9cdb181948",
   "metadata": {},
   "source": [
    "### **Combine the images in 'test-jpg' folder with images in 'test-jpg-additional' folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6423f714-1967-4212-ab16-e083c9bba8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sbp\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3667131b-b859-4cff-8618-f336b7736bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/notebooks/Hamoye-Internship/StageD/Data/planet/test-jpg-additional'\n",
    "fol = os.listdir('/notebooks/Hamoye-Internship/StageD/Data/planet/test-jpg-additional')\n",
    "p2 = '/notebooks/Hamoye-Internship/StageD/Data/planet/test-jpg'\n",
    "\n",
    "for i in fol:\n",
    "    p1 = os.path.join(path,i)\n",
    "    p3 = 'cp -r ' + p1 +' ' + p2+'/.'\n",
    "    sbp.Popen(p3,shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b1145-ee61-4cfa-9e07-0c9feb0e7210",
   "metadata": {},
   "source": [
    "#### **Import the required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51b6531-3f56-4aea-b985-c2d1f0a5f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, InputLayer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, ReduceLROnPlateau\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.backend import clear_session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eda3e0-b837-4298-aab9-a25c40e56c61",
   "metadata": {},
   "source": [
    "**Initialize the Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa6418f-77bc-4569-902a-82ba10d1b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (128, 128, 3) # Image Dimensions\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT_RATE = 0.5\n",
    "EPOCHS = 24\n",
    "LR = 0.0001 # Learning Rate\n",
    "REG_STRENGTH = 0.01 # Regularization Strength\n",
    "NFOLDS = 5 # No of folds for cross validation\n",
    "WORKERS = 4 # Multithreading no of threads\n",
    "MAXQ = 10 # Max Queue size for multithreading\n",
    "THRES = [0.2] * 17 # Threshold for truth value of label, applied on sigmoid output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e349ba-e46b-4a58-8c17-11d3a05c6a5e",
   "metadata": {},
   "source": [
    "**Create the path string for the images and csv files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e542fa73-bce5-4f29-87ee-49a3851ab1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '/notebooks/Hamoye-Internship/StageD/Data/planet/train-jpg'\n",
    "TEST_PATH = '/notebooks/Hamoye-Internship/StageD/Data/planet/test-jpg'\n",
    "\n",
    "TRAIN_CSV_PATH = '/notebooks/Hamoye-Internship/StageD/Data/planet/train_classes.csv'\n",
    "TEST_CSV_PATH = '/notebooks/Hamoye-Internship/StageD/Data/planet/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd402205-767d-4746-b913-edf757639029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61191"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the number of images in the test-jpg folder\n",
    "len(os.listdir(TEST_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b3119-444f-4b8d-98e7-67abadd6f2eb",
   "metadata": {},
   "source": [
    "**Construct dataframes holding training and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71eaad23-c0cd-40af-b316-a3877bf7d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_CSV_PATH)\n",
    "df_test = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "df_train['image_name'] = df_train['image_name'].astype(str) + '.jpg'\n",
    "df_test['image_name'] = df_test['image_name'].astype(str) + '.jpg'\n",
    "\n",
    "df_test['tags'] = df_test['tags'].apply(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06ee754d-1eec-4991-8ad0-d03bc25c4699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0.jpg</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1.jpg</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2.jpg</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3.jpg</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4.jpg</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_name                                       tags\n",
       "0  train_0.jpg                               haze primary\n",
       "1  train_1.jpg            agriculture clear primary water\n",
       "2  train_2.jpg                              clear primary\n",
       "3  train_3.jpg                              clear primary\n",
       "4  train_4.jpg  agriculture clear habitation primary road"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bae4bd73-4d3a-497d-9ec8-38016ae73c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0.jpg</td>\n",
       "      <td>[primary, clear, agriculture, road, water]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1.jpg</td>\n",
       "      <td>[primary, clear, agriculture, road, water]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2.jpg</td>\n",
       "      <td>[primary, clear, agriculture, road, water]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3.jpg</td>\n",
       "      <td>[primary, clear, agriculture, road, water]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4.jpg</td>\n",
       "      <td>[primary, clear, agriculture, road, water]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_name                                        tags\n",
       "0  test_0.jpg  [primary, clear, agriculture, road, water]\n",
       "1  test_1.jpg  [primary, clear, agriculture, road, water]\n",
       "2  test_2.jpg  [primary, clear, agriculture, road, water]\n",
       "3  test_3.jpg  [primary, clear, agriculture, road, water]\n",
       "4  test_4.jpg  [primary, clear, agriculture, road, water]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ee7994c-9d62-4980-9c41-be4cb439fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_files = np.array(df_train['image_name'].tolist())\n",
    "X_train_files.reshape((X_train_files.shape[0], 1))\n",
    "\n",
    "y_train = np.array(df_train['tags'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08551f8a-61a4-4011-9d22-6a0d95fd70bb",
   "metadata": {},
   "source": [
    "**Create a list of the unique labels only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a891b1cd-71da-4a40-9706-14acbce247dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agriculture', 'artisinal_mine', 'bare_ground', 'blooming', 'blow_down', 'clear', 'cloudy', 'conventional_mine', 'cultivation', 'habitation', 'haze', 'partly_cloudy', 'primary', 'road', 'selective_logging', 'slash_burn', 'water']\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "\n",
    "for tag in df_train['tags'].values:\n",
    "    labels_in_tag = tag.split(' ')\n",
    "    for label in labels_in_tag:\n",
    "        if label not in labels:\n",
    "            labels.append(label)\n",
    "        \n",
    "labels.sort()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9ea30-826d-460c-8c3e-2fe107fb92ec",
   "metadata": {},
   "source": [
    "**Create a CNN model with the VGG architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c368893-6cbe-4af5-8fbb-33213452408d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(INPUT_SHAPE))\n",
    "    model.add(VGG16(weights='imagenet', include_top=False))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(17, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89c869c7-15fa-4cdd-b281-8c264df0703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4026e6e4-e8d8-4d5c-b9f0-b3ec7beb61ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 13:16:33.387736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 13:16:33.433760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 13:16:33.434154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 13:16:33.436208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 13:16:33.436573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 13:16:33.436851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 13:16:34.261076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 13:16:34.261323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 13:16:34.261557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 13:16:34.261742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15384 MB memory:  -> device: 0, name: Quadro P5000, pci bus id: 0000:00:05.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "58900480/58889256 [==============================] - 1s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, None, None, 512)   14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 17)                139281    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,853,969\n",
      "Trainable params: 14,853,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1b5a4-c35d-4f79-9e50-f411b8cb838a",
   "metadata": {},
   "source": [
    "**Callback function to measure the F2 score of the validation set after every epoch and save the best model accordingly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1362376-f6a7-4c36-8782-71d06627c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_score(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, \"int32\")\n",
    "    y_pred = tf.cast(tf.round(y_pred), \"int32\") # implicit 0.5 threshold via tf.round\n",
    "    y_correct = y_true * y_pred\n",
    "    sum_true = tf.reduce_sum(y_true, axis=1)\n",
    "    sum_pred = tf.reduce_sum(y_pred, axis=1)\n",
    "    sum_correct = tf.reduce_sum(y_correct, axis=1)\n",
    "    precision = sum_correct / sum_pred\n",
    "    recall = sum_correct / sum_true\n",
    "    f_score = 5 * precision * recall / (4 * precision + recall)\n",
    "    f_score = tf.where(tf.math.is_nan(f_score), tf.zeros_like(f_score), f_score)\n",
    "    \n",
    "    return tf.reduce_mean(f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9412f19d-229e-4a53-8b01-a0fd5cbb16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fold = 0\n",
    "\n",
    "y_test = []\n",
    "\n",
    "folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=1).split(X_train_files, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f4ce2-a34b-4a65-9ab0-2bd575b33c25",
   "metadata": {},
   "source": [
    "**Iterate through each fold and calculate the F2 scores of the validation set after each epoch**\n",
    "\n",
    "**Save the best F2 scores model for each cross validation iteration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a843c7e-38f2-40ea-8239-e266e4228042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32383 validated image filenames belonging to 17 classes.\n",
      "Found 8096 validated image filenames belonging to 17 classes.\n",
      "Found 61191 validated image filenames belonging to 17 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_342/2317630506.py:76: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, epochs=EPOCHS, validation_data=val_generator, callbacks=callbacks,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 13:22:39.212187: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253/253 [==============================] - 159s 588ms/step - loss: 0.1526 - f2_score: 0.8073 - val_loss: 0.1231 - val_f2_score: 0.8602 - lr: 1.0000e-04\n",
      "Epoch 2/24\n",
      "253/253 [==============================] - 109s 427ms/step - loss: 0.1168 - f2_score: 0.8618 - val_loss: 0.1080 - val_f2_score: 0.8797 - lr: 1.0000e-04\n",
      "Epoch 3/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.1077 - f2_score: 0.8715 - val_loss: 0.1050 - val_f2_score: 0.8802 - lr: 1.0000e-04\n",
      "Epoch 4/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.1042 - f2_score: 0.8775 - val_loss: 0.1000 - val_f2_score: 0.8907 - lr: 1.0000e-04\n",
      "Epoch 5/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.1012 - f2_score: 0.8821 - val_loss: 0.1015 - val_f2_score: 0.8920 - lr: 1.0000e-04\n",
      "Epoch 6/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.0987 - f2_score: 0.8853 - val_loss: 0.0952 - val_f2_score: 0.8978 - lr: 1.0000e-04\n",
      "Epoch 7/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.0974 - f2_score: 0.8864 - val_loss: 0.0967 - val_f2_score: 0.9011 - lr: 1.0000e-04\n",
      "Epoch 8/24\n",
      "253/253 [==============================] - 106s 412ms/step - loss: 0.0955 - f2_score: 0.8890 - val_loss: 0.0982 - val_f2_score: 0.9011 - lr: 1.0000e-04\n",
      "Epoch 9/24\n",
      "253/253 [==============================] - 106s 413ms/step - loss: 0.0945 - f2_score: 0.8903 - val_loss: 0.0948 - val_f2_score: 0.8916 - lr: 1.0000e-04\n",
      "Epoch 10/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.0936 - f2_score: 0.8912 - val_loss: 0.0968 - val_f2_score: 0.8965 - lr: 1.0000e-04\n",
      "Epoch 11/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0931 - f2_score: 0.8924 - val_loss: 0.0961 - val_f2_score: 0.8977 - lr: 1.0000e-04\n",
      "Epoch 12/24\n",
      "253/253 [==============================] - 106s 412ms/step - loss: 0.0911 - f2_score: 0.8951 - val_loss: 0.0960 - val_f2_score: 0.8983 - lr: 1.0000e-04\n",
      "Epoch 13/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0912 - f2_score: 0.8953 - val_loss: 0.0961 - val_f2_score: 0.9012 - lr: 1.0000e-04\n",
      "Epoch 14/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0900 - f2_score: 0.8955 - val_loss: 0.0923 - val_f2_score: 0.8986 - lr: 1.0000e-04\n",
      "Epoch 15/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0895 - f2_score: 0.8962 - val_loss: 0.0899 - val_f2_score: 0.9016 - lr: 1.0000e-04\n",
      "Epoch 16/24\n",
      "253/253 [==============================] - 106s 413ms/step - loss: 0.0894 - f2_score: 0.8967 - val_loss: 0.0905 - val_f2_score: 0.8959 - lr: 1.0000e-04\n",
      "Epoch 17/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0887 - f2_score: 0.8968 - val_loss: 0.0964 - val_f2_score: 0.9026 - lr: 1.0000e-04\n",
      "Epoch 18/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.0869 - f2_score: 0.8995 - val_loss: 0.0904 - val_f2_score: 0.9043 - lr: 1.0000e-04\n",
      "Epoch 19/24\n",
      "253/253 [==============================] - 105s 413ms/step - loss: 0.0862 - f2_score: 0.9006 - val_loss: 0.0949 - val_f2_score: 0.9025 - lr: 1.0000e-04\n",
      "Epoch 20/24\n",
      "253/253 [==============================] - 105s 410ms/step - loss: 0.0850 - f2_score: 0.9009 - val_loss: 0.0922 - val_f2_score: 0.8996 - lr: 1.0000e-04\n",
      "Epoch 21/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.0844 - f2_score: 0.9018 - val_loss: 0.0916 - val_f2_score: 0.9064 - lr: 1.0000e-04\n",
      "Epoch 22/24\n",
      "253/253 [==============================] - 105s 411ms/step - loss: 0.0838 - f2_score: 0.9027 - val_loss: 0.0964 - val_f2_score: 0.9020 - lr: 1.0000e-04\n",
      "Epoch 23/24\n",
      "253/253 [==============================] - 105s 411ms/step - loss: 0.0841 - f2_score: 0.9022 - val_loss: 0.1000 - val_f2_score: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 24/24\n",
      "253/253 [==============================] - 106s 416ms/step - loss: 0.0820 - f2_score: 0.9045 - val_loss: 0.0909 - val_f2_score: 0.9069 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_342/2317630506.py:81: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  p_test = model.predict_generator(test_generator, workers=WORKERS, use_multiprocessing=True, max_queue_size=MAXQ)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32383 validated image filenames belonging to 17 classes.\n",
      "Found 8096 validated image filenames belonging to 17 classes.\n",
      "Found 61191 validated image filenames belonging to 17 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_342/2317630506.py:76: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, epochs=EPOCHS, validation_data=val_generator, callbacks=callbacks,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "253/253 [==============================] - 150s 579ms/step - loss: 0.1563 - f2_score: 0.8007 - val_loss: 0.1218 - val_f2_score: 0.8545 - lr: 1.0000e-04\n",
      "Epoch 2/24\n",
      "253/253 [==============================] - 107s 419ms/step - loss: 0.1163 - f2_score: 0.8624 - val_loss: 0.1063 - val_f2_score: 0.8745 - lr: 1.0000e-04\n",
      "Epoch 3/24\n",
      "253/253 [==============================] - 108s 421ms/step - loss: 0.1092 - f2_score: 0.8712 - val_loss: 0.1032 - val_f2_score: 0.8862 - lr: 1.0000e-04\n",
      "Epoch 4/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.1040 - f2_score: 0.8777 - val_loss: 0.0967 - val_f2_score: 0.8882 - lr: 1.0000e-04\n",
      "Epoch 5/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.1011 - f2_score: 0.8822 - val_loss: 0.0949 - val_f2_score: 0.8980 - lr: 1.0000e-04\n",
      "Epoch 6/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0991 - f2_score: 0.8846 - val_loss: 0.0943 - val_f2_score: 0.8921 - lr: 1.0000e-04\n",
      "Epoch 7/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0980 - f2_score: 0.8846 - val_loss: 0.0997 - val_f2_score: 0.8932 - lr: 1.0000e-04\n",
      "Epoch 8/24\n",
      "253/253 [==============================] - 106s 416ms/step - loss: 0.0956 - f2_score: 0.8882 - val_loss: 0.0926 - val_f2_score: 0.8980 - lr: 1.0000e-04\n",
      "Epoch 9/24\n",
      "253/253 [==============================] - 108s 420ms/step - loss: 0.0953 - f2_score: 0.8906 - val_loss: 0.0950 - val_f2_score: 0.8998 - lr: 1.0000e-04\n",
      "Epoch 10/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.0939 - f2_score: 0.8919 - val_loss: 0.0929 - val_f2_score: 0.8923 - lr: 1.0000e-04\n",
      "Epoch 11/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.0927 - f2_score: 0.8926 - val_loss: 0.0929 - val_f2_score: 0.8986 - lr: 1.0000e-04\n",
      "Epoch 12/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.0931 - f2_score: 0.8924 - val_loss: 0.0946 - val_f2_score: 0.8994 - lr: 1.0000e-04\n",
      "Epoch 13/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0906 - f2_score: 0.8948 - val_loss: 0.0916 - val_f2_score: 0.8993 - lr: 1.0000e-04\n",
      "Epoch 14/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.0902 - f2_score: 0.8956 - val_loss: 0.0963 - val_f2_score: 0.8940 - lr: 1.0000e-04\n",
      "Epoch 15/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0893 - f2_score: 0.8967 - val_loss: 0.0938 - val_f2_score: 0.9035 - lr: 1.0000e-04\n",
      "Epoch 16/24\n",
      "253/253 [==============================] - 106s 416ms/step - loss: 0.0881 - f2_score: 0.8979 - val_loss: 0.0910 - val_f2_score: 0.9010 - lr: 1.0000e-04\n",
      "Epoch 17/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.0874 - f2_score: 0.8984 - val_loss: 0.0911 - val_f2_score: 0.9008 - lr: 1.0000e-04\n",
      "Epoch 18/24\n",
      "253/253 [==============================] - 108s 420ms/step - loss: 0.0866 - f2_score: 0.8997 - val_loss: 0.0951 - val_f2_score: 0.9031 - lr: 1.0000e-04\n",
      "Epoch 19/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0860 - f2_score: 0.9002 - val_loss: 0.0943 - val_f2_score: 0.8990 - lr: 1.0000e-04\n",
      "Epoch 20/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0855 - f2_score: 0.9013 - val_loss: 0.0915 - val_f2_score: 0.8943 - lr: 1.0000e-04\n",
      "Epoch 21/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0846 - f2_score: 0.9025 - val_loss: 0.0960 - val_f2_score: 0.8956 - lr: 1.0000e-04\n",
      "Epoch 22/24\n",
      "253/253 [==============================] - 108s 419ms/step - loss: 0.0839 - f2_score: 0.9030 - val_loss: 0.0945 - val_f2_score: 0.9051 - lr: 1.0000e-04\n",
      "Epoch 23/24\n",
      "253/253 [==============================] - 106s 416ms/step - loss: 0.0830 - f2_score: 0.9032 - val_loss: 0.0972 - val_f2_score: 0.8966 - lr: 1.0000e-04\n",
      "Epoch 24/24\n",
      "253/253 [==============================] - 107s 419ms/step - loss: 0.0821 - f2_score: 0.9057 - val_loss: 0.0925 - val_f2_score: 0.9021 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_342/2317630506.py:81: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  p_test = model.predict_generator(test_generator, workers=WORKERS, use_multiprocessing=True, max_queue_size=MAXQ)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32383 validated image filenames belonging to 17 classes.\n",
      "Found 8096 validated image filenames belonging to 17 classes.\n",
      "Found 61191 validated image filenames belonging to 17 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_342/2317630506.py:76: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, epochs=EPOCHS, validation_data=val_generator, callbacks=callbacks,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "253/253 [==============================] - 134s 516ms/step - loss: 0.1669 - f2_score: 0.7872 - val_loss: 0.1248 - val_f2_score: 0.8518 - lr: 1.0000e-04\n",
      "Epoch 2/24\n",
      "253/253 [==============================] - 107s 419ms/step - loss: 0.1203 - f2_score: 0.8560 - val_loss: 0.1069 - val_f2_score: 0.8733 - lr: 1.0000e-04\n",
      "Epoch 3/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.1108 - f2_score: 0.8684 - val_loss: 0.1049 - val_f2_score: 0.8851 - lr: 1.0000e-04\n",
      "Epoch 4/24\n",
      "253/253 [==============================] - 106s 413ms/step - loss: 0.1061 - f2_score: 0.8753 - val_loss: 0.1004 - val_f2_score: 0.8847 - lr: 1.0000e-04\n",
      "Epoch 5/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.1026 - f2_score: 0.8793 - val_loss: 0.0986 - val_f2_score: 0.8960 - lr: 1.0000e-04\n",
      "Epoch 6/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.1006 - f2_score: 0.8829 - val_loss: 0.0986 - val_f2_score: 0.8917 - lr: 1.0000e-04\n",
      "Epoch 7/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0986 - f2_score: 0.8858 - val_loss: 0.0988 - val_f2_score: 0.8960 - lr: 1.0000e-04\n",
      "Epoch 8/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.0969 - f2_score: 0.8876 - val_loss: 0.0997 - val_f2_score: 0.8930 - lr: 1.0000e-04\n",
      "Epoch 9/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0957 - f2_score: 0.8889 - val_loss: 0.0932 - val_f2_score: 0.8900 - lr: 1.0000e-04\n",
      "Epoch 10/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0940 - f2_score: 0.8906 - val_loss: 0.0943 - val_f2_score: 0.8973 - lr: 1.0000e-04\n",
      "Epoch 11/24\n",
      "253/253 [==============================] - 107s 418ms/step - loss: 0.0936 - f2_score: 0.8921 - val_loss: 0.0963 - val_f2_score: 0.8975 - lr: 1.0000e-04\n",
      "Epoch 12/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.0921 - f2_score: 0.8935 - val_loss: 0.0946 - val_f2_score: 0.8895 - lr: 1.0000e-04\n",
      "Epoch 13/24\n",
      "253/253 [==============================] - 106s 413ms/step - loss: 0.0914 - f2_score: 0.8938 - val_loss: 0.0929 - val_f2_score: 0.8970 - lr: 1.0000e-04\n",
      "Epoch 14/24\n",
      "253/253 [==============================] - 107s 418ms/step - loss: 0.0907 - f2_score: 0.8955 - val_loss: 0.0906 - val_f2_score: 0.9027 - lr: 1.0000e-04\n",
      "Epoch 15/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0901 - f2_score: 0.8955 - val_loss: 0.0939 - val_f2_score: 0.9054 - lr: 1.0000e-04\n",
      "Epoch 16/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.0893 - f2_score: 0.8959 - val_loss: 0.0909 - val_f2_score: 0.8992 - lr: 1.0000e-04\n",
      "Epoch 17/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0881 - f2_score: 0.8977 - val_loss: 0.0923 - val_f2_score: 0.9036 - lr: 1.0000e-04\n",
      "Epoch 18/24\n",
      "253/253 [==============================] - 106s 413ms/step - loss: 0.0872 - f2_score: 0.8987 - val_loss: 0.0955 - val_f2_score: 0.8999 - lr: 1.0000e-04\n",
      "Epoch 19/24\n",
      "253/253 [==============================] - 106s 413ms/step - loss: 0.0866 - f2_score: 0.8999 - val_loss: 0.0955 - val_f2_score: 0.8963 - lr: 1.0000e-04\n",
      "Epoch 20/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0857 - f2_score: 0.9013 - val_loss: 0.0900 - val_f2_score: 0.9005 - lr: 1.0000e-04\n",
      "Epoch 21/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0848 - f2_score: 0.9018 - val_loss: 0.0930 - val_f2_score: 0.8998 - lr: 1.0000e-04\n",
      "Epoch 22/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0842 - f2_score: 0.9024 - val_loss: 0.0918 - val_f2_score: 0.9014 - lr: 1.0000e-04\n",
      "Epoch 23/24\n",
      "253/253 [==============================] - 106s 413ms/step - loss: 0.0835 - f2_score: 0.9038 - val_loss: 0.0957 - val_f2_score: 0.8990 - lr: 1.0000e-04\n",
      "Epoch 24/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.0831 - f2_score: 0.9038 - val_loss: 0.0908 - val_f2_score: 0.9020 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_342/2317630506.py:81: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  p_test = model.predict_generator(test_generator, workers=WORKERS, use_multiprocessing=True, max_queue_size=MAXQ)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32383 validated image filenames belonging to 17 classes.\n",
      "Found 8096 validated image filenames belonging to 17 classes.\n",
      "Found 61191 validated image filenames belonging to 17 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_342/2317630506.py:76: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, epochs=EPOCHS, validation_data=val_generator, callbacks=callbacks,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "253/253 [==============================] - 141s 543ms/step - loss: 0.1627 - f2_score: 0.7922 - val_loss: 0.1203 - val_f2_score: 0.8567 - lr: 1.0000e-04\n",
      "Epoch 2/24\n",
      "253/253 [==============================] - 108s 421ms/step - loss: 0.1182 - f2_score: 0.8592 - val_loss: 0.1094 - val_f2_score: 0.8778 - lr: 1.0000e-04\n",
      "Epoch 3/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.1100 - f2_score: 0.8700 - val_loss: 0.1057 - val_f2_score: 0.8759 - lr: 1.0000e-04\n",
      "Epoch 4/24\n",
      "253/253 [==============================] - 107s 419ms/step - loss: 0.1060 - f2_score: 0.8756 - val_loss: 0.0969 - val_f2_score: 0.8910 - lr: 1.0000e-04\n",
      "Epoch 5/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.1022 - f2_score: 0.8790 - val_loss: 0.1000 - val_f2_score: 0.8846 - lr: 1.0000e-04\n",
      "Epoch 6/24\n",
      "253/253 [==============================] - 107s 419ms/step - loss: 0.1008 - f2_score: 0.8830 - val_loss: 0.0988 - val_f2_score: 0.8928 - lr: 1.0000e-04\n",
      "Epoch 7/24\n",
      "253/253 [==============================] - 108s 420ms/step - loss: 0.0972 - f2_score: 0.8862 - val_loss: 0.0950 - val_f2_score: 0.8990 - lr: 1.0000e-04\n",
      "Epoch 8/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.0965 - f2_score: 0.8868 - val_loss: 0.0917 - val_f2_score: 0.8931 - lr: 1.0000e-04\n",
      "Epoch 9/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0946 - f2_score: 0.8899 - val_loss: 0.0918 - val_f2_score: 0.8943 - lr: 1.0000e-04\n",
      "Epoch 10/24\n",
      "253/253 [==============================] - 106s 416ms/step - loss: 0.0938 - f2_score: 0.8906 - val_loss: 0.0919 - val_f2_score: 0.8939 - lr: 1.0000e-04\n",
      "Epoch 11/24\n",
      "253/253 [==============================] - 107s 418ms/step - loss: 0.0931 - f2_score: 0.8922 - val_loss: 0.0910 - val_f2_score: 0.9015 - lr: 1.0000e-04\n",
      "Epoch 12/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0918 - f2_score: 0.8928 - val_loss: 0.0920 - val_f2_score: 0.9047 - lr: 1.0000e-04\n",
      "Epoch 13/24\n",
      "253/253 [==============================] - 108s 423ms/step - loss: 0.0923 - f2_score: 0.8922 - val_loss: 0.0979 - val_f2_score: 0.8999 - lr: 1.0000e-04\n",
      "Epoch 14/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.0906 - f2_score: 0.8949 - val_loss: 0.0944 - val_f2_score: 0.8973 - lr: 1.0000e-04\n",
      "Epoch 15/24\n",
      "253/253 [==============================] - 108s 420ms/step - loss: 0.0898 - f2_score: 0.8959 - val_loss: 0.0918 - val_f2_score: 0.8992 - lr: 1.0000e-04\n",
      "Epoch 16/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0896 - f2_score: 0.8955 - val_loss: 0.0923 - val_f2_score: 0.8992 - lr: 1.0000e-04\n",
      "Epoch 17/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.0883 - f2_score: 0.8973 - val_loss: 0.0958 - val_f2_score: 0.8924 - lr: 1.0000e-04\n",
      "Epoch 18/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0882 - f2_score: 0.8980 - val_loss: 0.0936 - val_f2_score: 0.8895 - lr: 1.0000e-04\n",
      "Epoch 19/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0869 - f2_score: 0.8995 - val_loss: 0.0923 - val_f2_score: 0.8979 - lr: 1.0000e-04\n",
      "Epoch 20/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.0860 - f2_score: 0.9010 - val_loss: 0.0912 - val_f2_score: 0.8988 - lr: 1.0000e-04\n",
      "Epoch 21/24\n",
      "253/253 [==============================] - 106s 413ms/step - loss: 0.0856 - f2_score: 0.9013 - val_loss: 0.0936 - val_f2_score: 0.9005 - lr: 1.0000e-04\n",
      "Epoch 22/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0840 - f2_score: 0.9017 - val_loss: 0.0907 - val_f2_score: 0.9009 - lr: 1.0000e-04\n",
      "Epoch 23/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.0840 - f2_score: 0.9021 - val_loss: 0.0952 - val_f2_score: 0.9030 - lr: 1.0000e-04\n",
      "Epoch 24/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0837 - f2_score: 0.9022 - val_loss: 0.0907 - val_f2_score: 0.9028 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_342/2317630506.py:81: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  p_test = model.predict_generator(test_generator, workers=WORKERS, use_multiprocessing=True, max_queue_size=MAXQ)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32384 validated image filenames belonging to 17 classes.\n",
      "Found 8095 validated image filenames belonging to 17 classes.\n",
      "Found 61191 validated image filenames belonging to 17 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_342/2317630506.py:76: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, epochs=EPOCHS, validation_data=val_generator, callbacks=callbacks,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "253/253 [==============================] - 141s 544ms/step - loss: 0.1540 - f2_score: 0.8050 - val_loss: 0.1250 - val_f2_score: 0.8611 - lr: 1.0000e-04\n",
      "Epoch 2/24\n",
      "253/253 [==============================] - 107s 418ms/step - loss: 0.1161 - f2_score: 0.8627 - val_loss: 0.1062 - val_f2_score: 0.8838 - lr: 1.0000e-04\n",
      "Epoch 3/24\n",
      "253/253 [==============================] - 108s 423ms/step - loss: 0.1085 - f2_score: 0.8709 - val_loss: 0.1015 - val_f2_score: 0.8904 - lr: 1.0000e-04\n",
      "Epoch 4/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.1040 - f2_score: 0.8772 - val_loss: 0.1003 - val_f2_score: 0.8883 - lr: 1.0000e-04\n",
      "Epoch 5/24\n",
      "253/253 [==============================] - 108s 421ms/step - loss: 0.1021 - f2_score: 0.8801 - val_loss: 0.0952 - val_f2_score: 0.8931 - lr: 1.0000e-04\n",
      "Epoch 6/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0997 - f2_score: 0.8822 - val_loss: 0.0967 - val_f2_score: 0.8928 - lr: 1.0000e-04\n",
      "Epoch 7/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0980 - f2_score: 0.8845 - val_loss: 0.0960 - val_f2_score: 0.8907 - lr: 1.0000e-04\n",
      "Epoch 8/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0965 - f2_score: 0.8872 - val_loss: 0.0939 - val_f2_score: 0.8979 - lr: 1.0000e-04\n",
      "Epoch 9/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0946 - f2_score: 0.8887 - val_loss: 0.0950 - val_f2_score: 0.8970 - lr: 1.0000e-04\n",
      "Epoch 10/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.0934 - f2_score: 0.8914 - val_loss: 0.0925 - val_f2_score: 0.9041 - lr: 1.0000e-04\n",
      "Epoch 11/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0927 - f2_score: 0.8917 - val_loss: 0.0985 - val_f2_score: 0.8915 - lr: 1.0000e-04\n",
      "Epoch 12/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.0922 - f2_score: 0.8930 - val_loss: 0.0907 - val_f2_score: 0.8894 - lr: 1.0000e-04\n",
      "Epoch 13/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.0911 - f2_score: 0.8942 - val_loss: 0.0906 - val_f2_score: 0.8996 - lr: 1.0000e-04\n",
      "Epoch 14/24\n",
      "253/253 [==============================] - 106s 416ms/step - loss: 0.0899 - f2_score: 0.8949 - val_loss: 0.0926 - val_f2_score: 0.8980 - lr: 1.0000e-04\n",
      "Epoch 15/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0889 - f2_score: 0.8967 - val_loss: 0.0921 - val_f2_score: 0.9014 - lr: 1.0000e-04\n",
      "Epoch 16/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0882 - f2_score: 0.8971 - val_loss: 0.0924 - val_f2_score: 0.9029 - lr: 1.0000e-04\n",
      "Epoch 17/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.0881 - f2_score: 0.8969 - val_loss: 0.0947 - val_f2_score: 0.8984 - lr: 1.0000e-04\n",
      "Epoch 18/24\n",
      "253/253 [==============================] - 108s 420ms/step - loss: 0.0871 - f2_score: 0.8991 - val_loss: 0.0956 - val_f2_score: 0.9036 - lr: 1.0000e-04\n",
      "Epoch 19/24\n",
      "253/253 [==============================] - 106s 414ms/step - loss: 0.0862 - f2_score: 0.8997 - val_loss: 0.0944 - val_f2_score: 0.9008 - lr: 1.0000e-04\n",
      "Epoch 20/24\n",
      "253/253 [==============================] - 107s 419ms/step - loss: 0.0851 - f2_score: 0.9017 - val_loss: 0.0911 - val_f2_score: 0.9053 - lr: 1.0000e-04\n",
      "Epoch 21/24\n",
      "253/253 [==============================] - 107s 416ms/step - loss: 0.0850 - f2_score: 0.9010 - val_loss: 0.0958 - val_f2_score: 0.8974 - lr: 1.0000e-04\n",
      "Epoch 22/24\n",
      "253/253 [==============================] - 106s 416ms/step - loss: 0.0843 - f2_score: 0.9025 - val_loss: 0.0919 - val_f2_score: 0.9009 - lr: 1.0000e-04\n",
      "Epoch 23/24\n",
      "253/253 [==============================] - 106s 415ms/step - loss: 0.0832 - f2_score: 0.9028 - val_loss: 0.0922 - val_f2_score: 0.9040 - lr: 1.0000e-04\n",
      "Epoch 24/24\n",
      "253/253 [==============================] - 107s 417ms/step - loss: 0.0821 - f2_score: 0.9048 - val_loss: 0.0966 - val_f2_score: 0.9064 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_342/2317630506.py:81: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  p_test = model.predict_generator(test_generator, workers=WORKERS, use_multiprocessing=True, max_queue_size=MAXQ)\n"
     ]
    }
   ],
   "source": [
    "for train_index, val_index in folds:\n",
    "    X_train_files_fold = X_train_files[train_index]\n",
    "    y_train_fold = y_train[train_index]\n",
    "    X_val_files_fold = X_train_files[val_index]\n",
    "    y_val_fold = np.array(y_train[val_index])\n",
    "    \n",
    "    train_df = pd.DataFrame(list(zip(X_train_files_fold, y_train_fold)), columns = ['image_name', 'tags'])\n",
    "    val_df = pd.DataFrame(list(zip(X_val_files_fold, y_val_fold)), columns = ['image_name', 'tags'])\n",
    "    \n",
    "    train_df['tags'] = train_df['tags'].apply(lambda x: x.split(' '))\n",
    "    val_df['tags'] = val_df['tags'].apply(lambda x: x.split(' '))\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True\n",
    "    )\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        train_df,\n",
    "        directory=TRAIN_PATH,\n",
    "        x_col='image_name',\n",
    "        y_col='tags',\n",
    "        target_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]),\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        classes=labels,\n",
    "    )\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(\n",
    "        rescale=1./255\n",
    "    )\n",
    "    \n",
    "    val_generator = val_datagen.flow_from_dataframe(\n",
    "        val_df,\n",
    "        directory=TRAIN_PATH,\n",
    "        x_col='image_name',\n",
    "        y_col='tags',\n",
    "        target_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]),\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        classes=labels,\n",
    "    )\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255\n",
    "    )\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        df_test,\n",
    "        directory=TEST_PATH,\n",
    "        x_col='image_name',\n",
    "        y_col='tags',\n",
    "        target_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]),\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        classes=labels,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    model_path_of_fold = os.path.join('', 'weights_of_fold_' + str(num_fold) + '.h5')\n",
    "    \n",
    "    clear_session()\n",
    "    model = create_model()\n",
    "    \n",
    "    adam = Adam(learning_rate=LR)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[f2_score])\n",
    "    \n",
    "    callbacks = [\n",
    "        ModelCheckpoint(model_path_of_fold, monitor='val_f2_score', save_best_only=True, mode='max'),\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3, mode='min', min_lr=0.000001)\n",
    "    ]\n",
    "    \n",
    "    model.fit_generator(train_generator, epochs=EPOCHS, validation_data=val_generator, callbacks=callbacks,\n",
    "                       workers=WORKERS, use_multiprocessing=True, max_queue_size=MAXQ)\n",
    "\n",
    "    model.load_weights(model_path_of_fold)\n",
    "\n",
    "    p_test = model.predict_generator(test_generator, workers=WORKERS, use_multiprocessing=True, max_queue_size=MAXQ)\n",
    "    y_test.append(p_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b884a-4d7c-4646-9a5a-dbf39dbed43a",
   "metadata": {},
   "source": [
    "**Take average of all predictions (OOF) generated during each fold of validation on the test set.\n",
    "Attach predictions to the test dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2c34691-d3f7-495b-a51a-8d3d8e0f8252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agriculture</th>\n",
       "      <th>artisinal_mine</th>\n",
       "      <th>bare_ground</th>\n",
       "      <th>blooming</th>\n",
       "      <th>blow_down</th>\n",
       "      <th>clear</th>\n",
       "      <th>cloudy</th>\n",
       "      <th>conventional_mine</th>\n",
       "      <th>cultivation</th>\n",
       "      <th>habitation</th>\n",
       "      <th>haze</th>\n",
       "      <th>partly_cloudy</th>\n",
       "      <th>primary</th>\n",
       "      <th>road</th>\n",
       "      <th>selective_logging</th>\n",
       "      <th>slash_burn</th>\n",
       "      <th>water</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013791</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.029518</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.995827</td>\n",
       "      <td>4.272908e-07</td>\n",
       "      <td>5.318260e-07</td>\n",
       "      <td>0.004644</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.005010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.034931</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.012601</td>\n",
       "      <td>0.014450</td>\n",
       "      <td>0.995650</td>\n",
       "      <td>1.806414e-07</td>\n",
       "      <td>2.356175e-06</td>\n",
       "      <td>0.028148</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.004561</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.003139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.024923</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>1.971538e-05</td>\n",
       "      <td>1.144057e-05</td>\n",
       "      <td>0.007933</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.999811</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.039902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.773282</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.934364</td>\n",
       "      <td>2.253891e-07</td>\n",
       "      <td>9.604149e-06</td>\n",
       "      <td>0.669514</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.077756</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.011414</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.044147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015437</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>1.505876e-01</td>\n",
       "      <td>7.611218e-07</td>\n",
       "      <td>0.004513</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.852313</td>\n",
       "      <td>0.846871</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.025278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agriculture  artisinal_mine  bare_ground  blooming  blow_down     clear  \\\n",
       "0     0.013791        0.000002     0.000083  0.029518   0.000438  0.995827   \n",
       "1     0.034931        0.000010     0.000436  0.012601   0.014450  0.995650   \n",
       "2     0.024923        0.000014     0.000315  0.000250   0.000397  0.000080   \n",
       "3     0.773282        0.000086     0.001505  0.003298   0.000528  0.934364   \n",
       "4     0.015437        0.000006     0.000134  0.000005   0.000008  0.000673   \n",
       "\n",
       "         cloudy  conventional_mine  cultivation  habitation      haze  \\\n",
       "0  4.272908e-07       5.318260e-07     0.004644    0.001454  0.001496   \n",
       "1  1.806414e-07       2.356175e-06     0.028148    0.001482  0.000116   \n",
       "2  1.971538e-05       1.144057e-05     0.007933    0.000959  0.000037   \n",
       "3  2.253891e-07       9.604149e-06     0.669514    0.004467  0.000945   \n",
       "4  1.505876e-01       7.611218e-07     0.004513    0.000794  0.000831   \n",
       "\n",
       "   partly_cloudy   primary      road  selective_logging  slash_burn     water  \n",
       "0       0.002401  0.999996  0.002821           0.003945    0.000043  0.005010  \n",
       "1       0.004561  0.999997  0.001493           0.002478    0.000297  0.003139  \n",
       "2       0.999811  0.999986  0.005967           0.000459    0.000086  0.039902  \n",
       "3       0.077756  0.999998  0.011414           0.001094    0.003309  0.044147  \n",
       "4       0.852313  0.846871  0.004713           0.000012    0.000007  0.025278  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.array(y_test[0])\n",
    "for i in range(1, NFOLDS):\n",
    "    result += np.array(y_test[i])\n",
    "result /= NFOLDS\n",
    "result = pd.DataFrame(result, columns = labels)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c76bd04-6d5f-4f9b-95f4-bdec043185a8",
   "metadata": {},
   "source": [
    "**Construct the csv file of predictions on test set, convert the binary labels to their respective labels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac551bbb-4c26-480e-b55a-7793b9713b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(result.shape[0]):\n",
    "    a = result.iloc[[i]]\n",
    "    a = a.apply(lambda x: x > THRES, axis=1)\n",
    "    a = a.transpose()\n",
    "    a = a.loc[a[i] == True]\n",
    "    ' '.join(list(a.index))\n",
    "    preds.append(' '.join(list(a.index)))\n",
    "    \n",
    "df_test['tags'] = preds\n",
    "df_test['image_name'] = df_test['image_name'].astype(str).str.slice(stop=-4)\n",
    "df_test.to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce21ec64-5805-404b-b2df-b4a90b298fb9",
   "metadata": {},
   "source": [
    "### **Summary of Work Done**\n",
    "\n",
    "- This is a multilabel classification problem. Hence, classes are encoded with binary values, each class gets a column containing truth value depicting if the image belongs to the label or not.\n",
    "\n",
    "- The dataset comprises of Images. Images have a large no of features, deep learning is particularly suited for these kinds of tasks.\n",
    "\n",
    "- Since images have a lot of local information, we can make do with lesser number of weights than a fully connected layer by using a CNN.\n",
    "\n",
    "- There is a huge class imbalance. Some labels occur on more than 37512 images, while some occur on as low as 99.\n",
    "Hence, I did not make a validation and train split, because some classes/combinations in that case may never appear in the training data due to randomness of split.\n",
    "\n",
    "- Instead, I used cross validation, and Out Of Fold(OOF) Approach. For every iteration of cross validation, I opbtained the sigmoid values on the test set and then averaged them all.\n",
    "Also, I used sigmoid activation in the output layer because softmax gives probabilities and is not used in multilabel classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5b8ffd-57fe-4f22-9052-2af79373e320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc513e3c-26da-4004-b535-45c46cec31b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df6e255a-c967-4ce4-b2f4-c40cdbabe186",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
